train_dataset <- quality$PoorCare[splitting_index,]
#create INDEX for training data set
splitting_index <- createDataPartition(quality$PoorCare , p=0.75 ,list=F)
splitting_index
#create INDEX for training data set
splitting_index <- createDataPartition(quality$PoorCare , p=0.75 ,list=F)
splitting_index
#create train and test dataset
train_dataset <- quality$PoorCare[splitting_index,]
#create train and test dataset
train_dataset <- quality[splitting_index,]
test_dataset <- quality[-splitting_index,]
View(train_dataset)
set.seed(1234)
#create INDEX for training data set
splitting_index <- createDataPartition(quality$PoorCare , p=0.75 ,list=F)
splitting_index
#create train and test dataset
train_dataset <- quality[splitting_index,]
test_dataset <- quality[-splitting_index,]
Logistic_model <- glm(data = train_dataset, PoorCare ~ OfficeVisits + Narcotics,family=binomial)
summary(Logistic_model)
# Make predictions on training set
predictTrain = predict(Logistic_model, type="response")
predictTrain
summary(predictTrain)
?predit()
?predict()
tapply(predictTrain, train_dataset$PoorCare, mean)
# Confusion matrix for threshold of 0.5
table(train_dataset$PoorCare, predictTrain > 0.5)
# Sensitivity and specificity
71/(71+16)
# Confusion matrix for threshold of 0.5
table(train_dataset$PoorCare, predictTrain > 0.5)
# Confusion matrix for threshold of 0.7
table(train_dataset$PoorCare, predictTrain > 0.7)
# Confusion matrix for threshold of 0.2
table(train_dataset$PoorCare, predictTrain > 0.2)
library(ROCR)
#ROC
library(ROCR)
#remove id column
quality <- quality[,-1]
# Look at structure
str(quality)
# Table outcome
table(quality$PoorCare)
# Baseline accuracy
98/131
set.seed(1234)
#create INDEX for training data set
splitting_index <- createDataPartition(quality$PoorCare , p=0.75 ,list=F)
splitting_index
#create train and test dataset
train_dataset <- quality[splitting_index,]
test_dataset <- quality[-splitting_index,]
#Create Logistic model
Logistic_model <- glm(data = train_dataset, PoorCare ~ OfficeVisits + Narcotics,family=binomial)
summary(Logistic_model)
# Make predictions on training set
predictTrain = predict(Logistic_model, type="response")
summary(predictTrain)
tapply(predictTrain, train_dataset$PoorCare, mean)
# Confusion matrix for threshold of 0.5
table(train_dataset$PoorCare, predictTrain > 0.5)
# Confusion matrix for threshold of 0.7
table(train_dataset$PoorCare, predictTrain > 0.7)
# Confusion matrix for threshold of 0.5
table(train_dataset$PoorCare, predictTrain > 0.5)
predictTrain
tapply(predictTrain, train_dataset$PoorCare, mean)
# Look at structure
str(quality)
# Prediction function
ROCRpred = prediction(predictTrain, train_dataset$PoorCare)
# Performance function
ROCRperf = performance(ROCRpred, "tpr", "fpr")
# Plot ROC curve
plot(ROCRperf)
# Add colors
plot(ROCRperf, colorize=TRUE)
# Add threshold labels
plot(ROCRperf, colorize=TRUE, print.cutoffs.at=seq(0,1,by=0.1), text.adj=c(-0.2,1.7))
# Prediction on Test Set
predictTest = predict(QualityLog, type = "response", newdata = qualityTest)
# Prediction on Test Set
predictTest = predict(test_dataset, type = "response", newdata = qualityTest)
# Prediction on Test Set
predictTest = predict(Logistic_model, type = "response", newdata = test_dataset)
table(qualityTest$PoorCare,predictTest >= 0.3)
table(test_dataset$PoorCare,predictTest >= 0.3)
# Confusion matrix for threshold of 0.5
comfuse_matrix_1 <- table(train_dataset$PoorCare, predictTrain > 0.5)
#Accuracy
acc_1 <-  (confuse_matrix_1[1]+confuse_matrix_1[4])/sum(confuse_matrix_1)
# Confusion matrix for threshold of 0.5
confuse_matrix_1 <- table(train_dataset$PoorCare, predictTrain > 0.5)
#Accuracy
acc_1 <-  (confuse_matrix_1[1]+confuse_matrix_1[4])/sum(confuse_matrix_1)
acc_1
# Confusion matrix for threshold of 0.7
confuse_matrix_2 <- table(train_dataset$PoorCare, predictTrain > 0.7)
acc_2 <-  (confuse_matrix_2[1]+confuse_matrix_2[4])/sum(confuse_matrix_2)
acc_2
# Confusion matrix for threshold of 0.2
confuse_matrix_3 <- table(train_dataset$PoorCare, predictTrain > 0.2)
acc_3 <-  (confuse_matrix_3[1]+confuse_matrix_3[4])/sum(confuse_matrix_3)
acc_3
?predict()
Logistic_model_2 <- glm(data = train_dataset, PoorCare ~ .,family=binomial)
summary(Logistic_model_2)
# Make predictions on training set (It's give a result in term of probability)
predictTrain = predict(Logistic_model_2, type="response")
summary(predictTrain)
# Make predictions on training set (It's give a result in term of probability)
predictTrain_2 = predict(Logistic_model_2, type="response")
confuse_matrix_1 <- table(train_dataset$PoorCare, predictTrain_2 > 0.5)
#Create Logistic model
Logistic_model <- glm(data = train_dataset, PoorCare ~ OfficeVisits + Narcotics,family=binomial)
# Make predictions on training set (It's give a result in term of probability)
predictTrain = predict(Logistic_model, type="response")
# Confusion matrix for threshold of 0.5
confuse_matrix_1 <- table(train_dataset$PoorCare, predictTrain > 0.5)
acc_1
confuse_matrix_1.2 <- table(train_dataset$PoorCare, predictTrain_2 > 0.5)
acc_1.2 <-  (confuse_matrix_1.2[1]+confuse_matrix_1.2[4])/sum(confuse_matrix_1.2)
acc_1.2
("Model 1 accuracy is" + acc_1)
print("Model 1 accuracy is" , acc_1)
acc_1
concat("Model 1 accuracy(threshold of 0.5) is" , acc_1)
paste("Model 1 accuracy(threshold of 0.5) is" , acc_1)
paste("Model 1 accuracy(threshold of 0.5) is" , round(acc_1,4))
paste("Model 2 accuracy(threshold of 0.5) is" , round(acc_1.2,4))
paste("Model 1 accuracy(threshold of 0.5) is" , round(acc_1,4))
paste("Model 2 accuracy(threshold of 0.5) is" , round(acc_1.2,4))
set.seed(1234)
# Read in dataset
quality = read.csv("quality.csv")
#remove id column
quality <- quality[,-1]
#create INDEX for training data set
splitting_index <- createDataPartition(quality$PoorCare , p=0.75 ,list=F)
View(quality)
#create INDEX for training data set
splitting_index <- createDataPartition(quality$PoorCare , p=0.75 ,list=F)
splitting_index
#create INDEX for training data set
splitting_index <- createDataPartition(quality$PoorCare , p=0.75 ,list=T)
splitting_index
library(caret)
library(caret)
set.seed(1234)
# Read in dataset
quality = read.csv("quality.csv")
#remove id column
quality <- quality[,-1]
# Table outcome
table(quality$PoorCare)
#create INDEX for training data set
splitting_index <- createDataPartition(quality$PoorCare , p=0.75 ,list=T)
splitting_index
# Look at structure
str(quality)
classes(splitting_index)
classes(quality)
Classes[splitting_index]
classes[splitting_index]
?classes[]
?classes
??classesToAM
?classes
??classes
?predict()
library(caret)
set.seed(1234)
# Read in dataset
quality = read.csv("quality.csv")
library(caret)
set.seed(1234)
# Read in dataset
quality = read.csv("quality.csv")
#remove id column
quality <- quality[,-1]
# Look at structure
str(quality)
# Table outcome
table(quality$PoorCare)
# Baseline accuracy (for percentage of split data)
98/131
#create(split) INDEX for training data set only. [It's not split the data]
splitting_index <- createDataPartition(quality$PoorCare , p=0.75 ,list=T)
splitting_index
#create train and test dataset
train_dataset <- quality[splitting_index,]
test_dataset <- quality[-splitting_index,]
#create(split) INDEX for training data set only. [It's not split the data]
splitting_index <- createDataPartition(quality$PoorCare , p=0.75 ,list=T)
splitting_index
#create(split) INDEX for training data set only. [It's not split the data]
splitting_index <- createDataPartition(quality$PoorCare , p=0.75 ,list=F)
splitting_index
#create train and test dataset
train_dataset <- quality[splitting_index,]
test_dataset <- quality[-splitting_index,]
#create(split) INDEX for training data set only. [It's not split the data]
splitting_index <- createDataPartition(quality$PoorCare , p=0.75 ,list=T)
splitting_index
class(splitting_index)
#create(split) INDEX for training data set only. [It's not split the data]
splitting_index <- createDataPartition(quality$PoorCare , p=0.75 ,list=F)
class(splitting_index)
#create(split) INDEX for training data set only. [It's not split the data]
splitting_index <- createDataPartition(quality$PoorCare , p=0.75 ,list=F)
splitting_index
class(splitting_index)
#create train and test dataset
train_dataset <- quality[splitting_index,]
test_dataset <- quality[-splitting_index,]
# Confusion matrix for threshold of 0.5
confuse_matrix_1 <- table(train_dataset$PoorCare, predictTrain > 0.5)
confuse_matrix_1
# Confusion matrix for threshold of 0.7
confuse_matrix_2 <- table(train_dataset$PoorCare, predictTrain > 0.7)
confuse_matrix_2
# Sensitivity and specificity
7/(7+18)
confuse_matrix_2
confuse_matrix_3
# Confusion matrix for threshold of 0.5
confuse_matrix_1 <- table(train_dataset$PoorCare, predictTrain > 0.5)
confuse_matrix_1
?prediction()
confusionMatrix(data = predictTrain , train_dataset$PoorCare)
confusionMatrix(data = predictTrain , reference = "PoorCare")
confusionMatrix(data = predictTrain , reference = train_dataset$PoorCare)
confusionMatrix(data =predictTrain)
confusionMatrix(data =predictTrain , reference = train_dataset)
confusionMatrix(data =predictTrain , reference = train_dataset$PoorCare)
confusionMatrix(data =predictTrain , reference = PoorCare)
confusionMatrix(data =predictTrain , reference = "PoorCare")
confusionMatrix(data = predictTrain , reference = train_dataset$PoorCare)
class( train_dataset$PoorCare)
?confusionMatrix()
confusionMatrix(data = predictTrain , reference = as.factor(train_dataset$PoorCare))
temp <- as.factor(train_dataset$PoorCare)
confusionMatrix(data = predictTrain , reference = temp)
predictTrain
train_dataset$PoorCare
count(train_dataset$PoorCare)
temp
predictTrain
str(train_dataset$PoorCare)
str(predictTrain)
max(train_dataset$PoorCare)
nrow(train_dataset$PoorCare)
confusionMatrix(data =  train_dataset$PoorCare , reference = predictTrain)
confusionMatrix(data =  train_dataset , reference = predictTrain)
class(predictTrain)
confusionMatrix(data =  train_dataset$PoorCare , reference = as.factor(predictTrain))
confusionMatrix(train_dataset$PoorCare , as.factor(as.integer(predictTrain)))
confusionMatrix(as.factors(train_dataset$PoorCare) , as.factor(as.integer(predictTrain)))
confusionMatrix(as.factor(train_dataset$PoorCare) , as.factor(as.integer(predictTrain)))
predictTrain
confusionMatrix(as.factor(train_dataset$PoorCare) , as.factor(as.integer(predictTrain>0.5)))
predictTrain
predictTrain>0.5
confusionMatrix(train_dataset$PoorCare , as.factor(as.integer(predictTrain>0.5)))
confusionMatrix(as.factor(train_dataset$PoorCare) , as.factor(as.integer(predictTrain>0.9)))
confusionMatrix(as.factor(train_dataset$PoorCare) , as.factor(as.integer(predictTrain>0.5)))
#import data
Original_data <- read.csv("drought_target.csv",header=T)
setwd("~/Documents/Data Science Master/ANA625/Final/Final_ANA625")
#import data
Original_data <- read.csv("drought_target.csv",header=T)
names(Original_data)
colSums(is.na(Original_data))
which(is.na(Original_data))
names(Original_data)
missing_data_index <- which(is.na(Original_data))
missing_data_index
class(missing_data_index)
missing_data_index[2]
Original_data[missing_data_index,]
missing.index <- as.factor(missing_data_index)
Original_data[missing.index,]
view(Original_data)
Views(Original_data)
views(Original_data)
View(Original_data)
missing.index
Original_data[24336,]
str(Original_data)
as.Date(Original_data$ValidEnd)
as.Date.character(Original_data$ValidEnd)
#Data Pre processing
names(Original_data)
str(Original_data)
as.Date(Original_data$ValidEnd,format="%m/%d/%Y")
?is.na()
is.na(Original_data[1,])
is.na(Original_data[24336,])
nrow(Original_data)
nrow(Original_data[,1])
nrow(Original_data[,3])
Original_data[,3]
Original_data$ValidEnd
class(Original_data$ValidEnd)
Original_data$ValidEnd <- as.Date(Original_data$ValidEnd,format="%m/%d/%Y")
class(Original_data$ValidEnd)
Original_data$ValidStart <- as.Date(Original_data$ValidStart,format="%m/%d/%Y")
str(Original_data)
colnames(Original_data)
colnames(Original_data)[1] <- temp
colnames(Original_data)[1] <- "temp"
colnames(Original_data)[1]
str(Original_data)
colnames(Original_data)[1] <- "ID"
str(Original_data)
#Data Pre processing
names(Original_data)
#Checking for missing value
colSums(is.na(Original_data))
which(is.na(Original_data)
which(is.na(Original_data))
missing_data_index <- which(is.na(Original_data))
missing_data_index
#Checking for missing value
is.na(Original_data)
#Checking for missing value
sum(is.na(Original_data))
#Checking for missing value
sum(is.na(Original_data))
colSums(is.na(Original_data))
missing_data_index
?which()
which(is.na(Original_data))
80/40
missing.index <- as.factor(which(is.na(Original_data)))  #Missing value row index
missing.index
Original_data[missing.index,] #show missing value rows
missing.index
Original_data[24336,] #show missing value rows
class(missing.index)
as.list(missing.index)
Original_data[as.list(missing.index),] #show missing value rows
missing.index <- as.factor(which(is.na(Original_data)))  #Missing value row index
Original_data[missing.index,] #show missing value rows
missing.index
is.na(Original_data)
is.na(Original_data) == T
temp <- (is.na(Original_data) == T)
temp
temp <- (is.na(Original_data) == TRUE)
View(Original_data)
temp
is.na(Original_data)
temp <- (is.na(Original_data$republican) == TRUE)
is.na(Original_data$republican) == TRUE
colSums(is.na(Original_data)) #Total missing value in each column
missing.index <- as.factor(which(is.na(Original_data)))  #Missing value row index
missing.index
Original_data[c(24336,24337),] #show missing value rows
Original_data[8008,] #show missing value rows
Orginal_data[111111,]
Original_data[111111,]
which(is.na(Original_data) #Missing value row index
which(is.na(Original_data)) #Missing value row index
which(is.na(Original_data))
which(is.na(Original_data), arr.ind=TRUE)
colSums(is.na(Original_data)) #Total missing value in each column
#Checking for missing value
sum(is.na(Original_data))  #Total missing value
sum(which(is.na(Original_data), arr.ind=TRUE))
colSums(which(is.na(Original_data), arr.ind=TRUE))
which(is.na(Original_data), arr.ind=TRUE)
tt <- which(is.na(Original_data), arr.ind=TRUE)
tt[1]
tt[,1]
unique(tt[,1])
dup_t <- unique(tt[,1])
nrow(dup_t)
nrows(dup_t)
max(dup_t)
levels(dup_t)
nlevels(dup_t)
nlevels(dup_t)
dup_t
#set up library
library(plyr)
count(dup_t)
class(dup_t)
as.factor(dup_t)
Original_data[as.factor(dup_t),]
#Checking for transformation
ggplot(Original_data,aes(x = target)) + geom_bar() + ggtitle("Amount of each type of glass")
library(ggplot2)
#Checking for transformation
ggplot(Original_data,aes(x = target)) + geom_bar() + ggtitle("Amount of each type of glass")
#Checking for transformation
ggplot(Original_data,aes(x = target,fill=target)) + geom_bar() + ggtitle("Amount of each type of glass")
scale_fill_manual("Target", values = c("Blue" = "blue", "Red" = "red") + ggtitle("Target amount")
#Checking for transformation
ggplot(Original_data,aes(x = target,fill=target)) + geom_bar() + scale_fill_manual("Target", values = c("Blue" = "blue", "Red" = "red") + ggtitle("Target amount")
#Checking for transformation
ggplot(Original_data,aes(x = target)) + geom_bar() + scale_fill_manual("Target", values = c("Blue" = "blue", "Red" = "red") + ggtitle("Target amount")
#Checking for transformation
ggplot(Original_data,aes(x = target)) + geom_bar() + scale_fill_manual("Target", values = c("Blue" = "blue", "Red" = "red")) + ggtitle("Target amount")
#Checking for transformation
ggplot(Original_data,aes(x = target , fill=target)) + geom_bar() + scale_fill_manual("target", values = c("Blue" = "blue", "Red" = "red")) + ggtitle("Target amount")
ggplot(Original_data,aes(x = D0)) + geom_histogram(bins=50) + ggtitle("RI Distribution")
ggplot(Original_data,aes(x = D0)) + geom_histogram() + ggtitle("RI Distribution")
bins=50
ggplot(Original_data,aes(x = D0)) + geom_histogram(bins=50) + ggtitle("RI Distribution")
ggplot(Original_data,aes(x = D0)) + geom_histogram(bins=50) + ggtitle("D0 Distribution")
ggplot(Original_data,aes(x = D1)) + geom_histogram(bins=50) + ggtitle("D0 Distribution")
ggplot(Original_data,aes(x = D2)) + geom_histogram(bins=50) + ggtitle("D0 Distribution")
ggplot(Original_data,aes(x = D3)) + geom_histogram(bins=50) + ggtitle("D0 Distribution")
ggplot(Original_data,aes(x = D4)) + geom_histogram(bins=50) + ggtitle("D0 Distribution")
ggplot(Original_data,aes(x = None)) + geom_histogram(bins=50) + ggtitle("D0 Distribution")
max(Original_data$None)
str(Original_data$Tar)
str(Original_data$Target)
Original_data$Target
Original_data$None
Original_data
Original_data$target
str(Original_data)
levels(Original_data$target)
levels(Original_data$target) <- c("0","1")
levels(Original_data$target)
Original_data$target
str(Original_data)
Original_data[!!rowSums(!is.na(Original_data)),]
Original_data[Original_data$state == "pr",]
Original_data[,Original_data$state == "pr"]
Original_data[,Original_data$state = "pr"]
Original_data[,Original_data$state == "PR"]
Original_data[Original_data$state == "PR",]
nrow(Original_data[Original_data$state == "PR",])
is.na(Original_data)
nrow(Original_data[Original_data$state == "PR",])
colSums(is.na(Original_data))
na.omit(Original_data)
data_nona <- na.omit(Original_data)
sum(is.na(data_nona))
colSums(is.na(data_nona))
(is.na(data_nona)) #check for the missing value again
(is.na(data_nona)) #check for the missing value again
sum(is.na(data_nona)) #check for the missing value again
sum(is.na(Original_data)) #Total missing value in each column
missing.value.index <- as.factor(which(is.na(Original_data))) #Missing value row index
missing.value.index
is.na(Original_data)
Original_data[(is.na(Original_data)),]
Original_data[(!is.na(Original_data)),]
#import data
Original_data <- read.csv("drought_target.csv",header=T)
Original_data[(!is.na(Original_data)),]
Original_data[(is.na(Original_data)),]
is.na(Original_data)
is.na(Original_data) == T
is.na(Original_data)
rowSums(is.na(Original_data))
rowSums(is.na(~Original_data))
rowSums(is.na(!Original_data))
rowSums(!is.na(Original_data))
ggplot(Original_data,aes(x = D0)) + geom_histogram(bins=50) + ggtitle("D0 Distribution")
ggplot(Original_data,aes(x = D1)) + geom_histogram(bins=50) + ggtitle("D0 Distribution")
ggplot(Original_data,aes(x = D3)) + geom_histogram(bins=50) + ggtitle("D0 Distribution")
ggplot(Original_data,aes(x = D0)) + geom_histogram(bins=50) + ggtitle("D0 Distribution")
ggplot(Original_data,aes(x = log10(D0)) + geom_histogram(bins=50) + ggtitle("D0 Distribution"))
ggplot(Original_data,aes(x = log10(D0))) + geom_histogram(bins=50) + ggtitle("D0 Distribution")
ggplot(Original_data,aes(x = log2(D0))) + geom_histogram(bins=50) + ggtitle("D0 Distribution")
ggplot(Original_data,aes(x = log10(D0))) + geom_histogram(bins=50) + ggtitle("D0 Distribution")
ggplot(Original_data,aes(x = D1)) + geom_histogram(bins=50) + ggtitle("D0 Distribution")
ggplot(Original_data,aes(x = log10(D1)) + geom_histogram(bins=50) + ggtitle("D0 Distribution")
ggplot(Original_data,aes(x = log10(D1))) + geom_histogram(bins=50) + ggtitle("D0 Distribution")
ggplot(Original_data,aes(x = log10(D1))) + geom_histogram(bins=50) + ggtitle("D0 Distribution")
ggplot(Original_data,aes(x = None)) + geom_histogram(bins=50) + ggtitle("None Distribution")
ggplot(Original_data,aes(x = log10(None))) + geom_histogram(bins=50) + ggtitle("None Distribution")
ggplot(Original_data,aes(x = D0)) + geom_histogram(bins=50) + ggtitle("D0 Distribution")
ggplot(Original_data,aes(x = log10(D0+1))) + geom_histogram(bins=50) + ggtitle("D0 Distribution")
ggplot(Original_data,aes(x = log10(D0))) + geom_histogram(bins=50) + ggtitle("D0 Distribution")
ggplot(Original_data,aes(x = log(D0))) + geom_histogram(bins=50) + ggtitle("D0 Distribution")
ggplot(Original_data,aes(x = D0)) + geom_histogram(bins=50) + ggtitle("D0 Distribution")
ggplot(Original_data,aes(x = log(D0))) + geom_histogram(bins=50) + ggtitle("D0 Distribution")
log(0)
log(1)
log(1.2)
ggplot(Original_data,aes(x = log(D0))) + geom_histogram(bins=50) + ggtitle("D0 Distribution")
ggplot(Original_data,aes(x = log10(D0))) + geom_histogram(bins=50) + ggtitle("D0 Distribution")
summary(Original_data$D0)
summary(log(Original_data$D0))
summary(log(Original_data$D0+1))
ggplot(Original_data,aes(x = D2)) + geom_histogram(bins=50) + ggtitle("D2 Distribution")
ggplot(Original_data,aes(x = log(D2)) + geom_histogram(bins=50) + ggtitle("D2 Distribution")
ggplot(Original_data,aes(x = log(D2))) + geom_histogram(bins=50) + ggtitle("D2 Distribution")
ggplot(Original_data,aes(x = log(D2))) + geom_histogram(bins=50) + ggtitle("D2 Distribution")
ggplot(Original_data,aes(x = D3)) + geom_histogram(bins=50) + ggtitle("D3 Distribution")
ggplot(Original_data,aes(x = log(D3))) + geom_histogram(bins=50) + ggtitle("D3 Distribution")
ggplot(Original_data,aes(x = D3)) + geom_histogram(bins=50) + ggtitle("D3 Distribution")
ggplot(Original_data,aes(x = D4)) + geom_histogram(bins=50) + ggtitle("D4 Distribution")
ggplot(Original_data,aes(x = log(D4))) + geom_histogram(bins=50) + ggtitle("D4 Distribution")
ggplot(Original_data,aes(x = None)) + geom_histogram(bins=50) + ggtitle("None Distribution")
ggplot(Original_data,aes(x = log10(None))) + geom_histogram(bins=50) + ggtitle("None Distribution")
ggplot(Original_data,aes(x = None)) + geom_histogram(bins=50) + ggtitle("None Distribution")
ggplot(Original_data,aes(x = log10(None))) + geom_histogram(bins=50) + ggtitle("None Distribution")
ggplot(Original_data,aes(x = None)) + geom_histogram(bins=50) + ggtitle("None Distribution")
ggplot(Original_data,aes(x = log10(None))) + geom_histogram(bins=50) + ggtitle("None Distribution")
str(Original_data)
