legend("bottomright",legend=c("Logistic","Random Forrest","KNN"),col=c("salmon","green","blue"),lwd=4,cex=0.9,
text.col = c("salmon","green","blue"),bty='n')
randomF_model <- train(target ~ D2 + D3 +  None , train_data ,
trControl = rf_trainControl ,
method = "ranger",
tuneLength = 4,
preProcess = c("center","scale"))
randomF_pred_roc <- predict(randomF_model,newdata=test_data,type="prob")
roc(test_data$target,randomF_pred_roc$blue,plot = T,print.auc=T,print.auc.y=.3,col="green",add=T)
roc(test_data$target,logis_pred_roc$blue,plot = T,print.auc=T,col="salmon",main="ROC Curve of 3 models")
roc(test_data$target,randomF_pred_roc$blue,plot = T,print.auc=T,print.auc.y=.3,col="green",add=T)
roc(test_data$target,knn_pred_roc$blue,plot = T,print.auc=T,col="blue",print.auc.y=.4,add=T)
#add legend
legend("bottomright",legend=c("Logistic","Random Forrest","KNN"),col=c("salmon","green","blue"),lwd=4,cex=0.9,
text.col = c("salmon","green","blue"),bty='n')
print(randomF_model)
#test data set
randomF_pred_test <- predict(randomF_model,newdata=test_data,type="raw")
summary(randomF_pred_test)
#Confusion Matrix with test data
randomF_cm <- confusionMatrix(as.factor(randomF_pred_test),test_data$target,mode='everything')
randomF_cm
logistic_model <- train(target ~ D2 + D3 + None , train_data,
trControl = trainControl ,
method = "glm",
preProcess = c("center","scale"))
logis_pred_roc <- predict(logistic_model,newdata=test_data,type="prob")
roc(test_data$target,logis_pred_roc$blue,plot = T,print.auc=T,col="salmon",main="ROC Curve of 3 models")
roc(test_data$target,randomF_pred_roc$blue,plot = T,print.auc=T,print.auc.y=.3,col="green",add=T)
roc(test_data$target,knn_pred_roc$blue,plot = T,print.auc=T,col="blue",print.auc.y=.4,add=T)
#add legend
legend("bottomright",legend=c("Logistic","Random Forrest","KNN"),col=c("salmon","green","blue"),lwd=4,cex=0.9,
text.col = c("salmon","green","blue"),bty='n')
roc(test_data$target,logis_pred_roc$blue,plot = T,print.auc=T,col="salmon",main="ROC Curve of 3 models")
roc(test_data$target,randomF_pred_roc$blue,plot = T,print.auc=T,print.auc.y=.3,col="green",add=T)
roc(test_data$target,knn_pred_roc$blue,plot = T,print.auc=T,col="blue",print.auc.y=.4,add=T)
#add legend
legend("bottomright",legend=c("Logistic","Random Forrest","KNN"),col=c("salmon","green","blue"),lwd=4,cex=0.9,
text.col = c("salmon","green","blue"),bty='n')
knn_model <- train(target ~  + D3 + D2 +None , train_data ,
trControl = trainControl,
method = "knn",
tuneLength = 20,
preProcess = c("center","scale"))
knn_pred_roc <- predict(knn_model,newdata=test_data,type="prob")
roc(test_data$target,logis_pred_roc$blue,plot = T,print.auc=T,col="salmon",main="ROC Curve of 3 models")
roc(test_data$target,randomF_pred_roc$blue,plot = T,print.auc=T,print.auc.y=.3,col="green",add=T)
roc(test_data$target,knn_pred_roc$blue,plot = T,print.auc=T,col="blue",print.auc.y=.4,add=T)
#add legend
legend("bottomright",legend=c("Logistic","Random Forrest","KNN"),col=c("salmon","green","blue"),lwd=4,cex=0.9,
text.col = c("salmon","green","blue"),bty='n')
roc(test_data$target,logis_pred_roc$blue,plot = T,print.auc=T,col="salmon",main="ROC Curve of 3 models")
roc(test_data$target,randomF_pred_roc$blue,plot = T,print.auc=T,print.auc.y=.4,col="green",add=T)
roc(test_data$target,knn_pred_roc$blue,plot = T,print.auc=T,col="blue",print.auc.y=.3,add=T)
#add legend
legend("bottomright",legend=c("Logistic","Random Forrest","KNN"),col=c("salmon","green","blue"),lwd=4,cex=0.9,
text.col = c("salmon","green","blue"),bty='n')
plot(varImp(logistic_model))
logistic_model <- train(target ~ D0 + D1 + D2 + D3 + D4 + None + state , train_data,
trControl = trainControl ,
method = "glm",
preProcess = c("center","scale"))
plot(varImp(logistic_model))
logistic_model <- train(target ~ D0 + D1 + D2 + D3 + D4 + None  , train_data,
trControl = trainControl ,
method = "glm",
preProcess = c("center","scale"))
plot(varImp(logistic_model))
randomF_model <- train(target ~D0 + D1 + D2 + D3 + D4+  None , train_data ,
trControl = rf_trainControl ,
method = "ranger",
tuneLength = 4,
preProcess = c("center","scale"))
plot(varImp(randomF_model))
varImp(randomF_model)
class(test_data$state)
#evaluate model
coef(logistic_model$finalModel)
#evaluate model
round(coef(logistic_model$finalModel),3)
logistic_model <- train(target ~ D0 + D1 + D2 + D3 + D4 + None  , train_data,
trControl = trainControl ,
method = "glm",
preProcess = c("center","scale"))
###################################
######### Random Forrest ##########
###################################
rf_trainControl <- trainControl(method="cv",number=5,classProbs=T,summaryFunction=twoClassSummary)
randomF_model <- train(target ~D0 + D1 + D2 + D3 + D4+  None , train_data ,
trControl = rf_trainControl ,
method = "ranger",
tuneLength = 4,
preProcess = c("center","scale"))
knn_model <- train(target ~ D0 + D1 + D3 + D2 + D4 + None , train_data ,
trControl = trainControl,
method = "knn",
tuneLength = 20,
preProcess = c("center","scale"))
knn_pred_train <- predict(knn_model,type="raw")
#test data set
knn_pred_test <- predict(knn_model,newdata=test_data,type="raw")
#Confusion Matrix with test data
knn_cm <- confusionMatrix(as.factor(knn_pred_test),test_data$target,mode='everything')
randomF_pred_test <- predict(randomF_model,newdata=test_data,type="raw")
#Confusion Matrix with test data
randomF_cm <- confusionMatrix(as.factor(randomF_pred_test),test_data$target,mode='everything')
#test data set
logistic_pred_test <- predict(logistic_model,newdata=test_data,type="raw")
#Confusion Matrix with test data
logis_cm <- confusionMatrix(as.factor(logistic_pred_test),test_data$target,mode='everything')
paste("F1 Score - Logistic: ",round(logis_cm$byClass[7],4),"| Random Forrest: ", round(randomF_cm$byClass[7],4),
"| KNN: ",round(knn_cm$byClass[7],4))
paste("Accuracy - Logistic: ",round(logis_cm$overall[1],4),"| Random Forrest: ", round(randomF_cm$overall[1],4),
"| KNN: ",round(knn_cm$overall[1],4))
paste("Kappa - Logistic: ",round(logis_cm$byClass[2],4),"| Random Forrest: ", round(randomF_cm$overall[2],4),
"| KNN: ",round(knn_cm$overall[2],4))
#AUC
knn_prediction <- prediction(as.numeric(knn_pred_test),test_data$target)
knn_auc <- performance(knn_prediction,"auc")
knn_auc_value <- knn_auc@y.values[[1]] #area under curve
rf_prediction <- prediction(as.numeric(randomF_pred_test),test_data$target)
randomF_auc <- performance(rf_prediction,"auc")
randomF_auc_value <- randomF_auc@y.values[[1]] #area under curve
#AUC
logistic_prediction <- prediction(as.numeric(logistic_pred_test),test_data$target)
logistic_auc <- performance(rf_prediction,"auc")
logistic_auc_value <- logistic_auc@y.values[[1]] #area under curve
paste("AUC - Logistic: ", round(logistic_auc_value,4), "| Random Forrest: ",round(randomF_auc_value,4),
"| KNN: ",round(knn_auc_value,4))
logis_pred_roc <- predict(logistic_model,newdata=test_data,type="prob")
randomF_pred_roc <- predict(randomF_model,newdata=test_data,type="prob")
knn_pred_roc <- predict(knn_model,newdata=test_data,type="prob")
roc(test_data$target,logis_pred_roc$blue,plot = T,print.auc=T,col="salmon",main="ROC Curve of 3 models")
roc(test_data$target,randomF_pred_roc$blue,plot = T,print.auc=T,print.auc.y=.4,col="green",add=T)
roc(test_data$target,knn_pred_roc$blue,plot = T,print.auc=T,col="blue",print.auc.y=.3,add=T)
#add legend
legend("bottomright",legend=c("Logistic","Random Forrest","KNN"),col=c("salmon","green","blue"),lwd=4,cex=0.9,
text.col = c("salmon","green","blue"),bty='n')
auc(roc(test_data$target,logis_pred_roc$blue)
)
source('~/Documents/Data Science Master/ANA625/Final/Final_ANA625/Final_project_ANA625_d2.R', echo=TRUE)
logictic_auc
logictic_auc <- auc(roc(test_data$target,logis_pred_roc$blue))
logictic_auc
randomF_auc
randomF_auc <- auc(roc(test_data$target,randomF_pred_roc$blue))
randomF_auc
knn_auc <- auc(roc(test_data$target,knn_pred_roc$blue))
knn_auc
paste("AUC - Logistic: ", logistic_auc, "| Random Forrest: ",randomF_auc,
"| KNN: ",knn_auc)
randomF_auc
paste("AUC - Logistic: ", logistic_auc, "| Random Forrest: ",randomF_auc,
"| KNN: ", knn_auc)
class(knn_auc)
knn_auc(2)
knn_auc[2]
knn_auc[1]
knn_auc[0]
knn_auc[1]
knn_auc
paste("AUC - Logistic: ", logistic_auc[1], "| Random Forrest: ",randomF_auc[1],
"| KNN: ", knn_auc[1])
logistic_auc[1]
logistic_auc
logictic_auc <- auc(roc(test_data$target,logis_pred_roc$blue))
logistic_auc
logictic_auc <- auc(roc(test_data$target,logis_pred_roc$blue))
logictic_auc
logistic_auc
logistic_auc <- auc(roc(test_data$target,logis_pred_roc$blue))
paste("AUC - Logistic: ", logistic_auc[1], "| Random Forrest: ",randomF_auc[1],
"| KNN: ", knn_auc[1])
paste("AUC - Logistic: ", round(logistic_auc[1],3), "| Random Forrest: ",round(randomF_auc[1],3),
"| KNN: ", round(knn_auc[1],3))
paste("AUC - Logistic: ", round(logistic_auc[1],3), "| Random Forrest: ",round(randomF_auc[1],3),
"| KNN: ", round(knn_auc[1],1))
paste("AUC - Logistic: ", round(logistic_auc[1],3), "| Random Forrest: ",round(randomF_auc[1],3),
"| KNN: ", round(knn_auc[1]))
paste("AUC - Logistic: ", round(logistic_auc[1],3), "| Random Forrest: ",round(randomF_auc[1],3),
"| KNN: ", knn_auc[1])
paste("AUC - Logistic: ", round(logistic_auc[1],3), "| Random Forrest: ",round(randomF_auc[1],3),
"| KNN: ", round(knn_auc[1],3))
roc(test_data$target,logis_pred_roc$blue,plot = T,print.auc=T,col="salmon",main="ROC Curve of 3 models")
roc(test_data$target,randomF_pred_roc$blue,plot = T,print.auc=T,print.auc.y=.4,col="green",add=T)
roc(test_data$target,knn_pred_roc$blue,plot = T,print.auc=T,col="blue",print.auc.y=.3,add=T)
#add legend
legend("bottomright",legend=c("Logistic","Random Forrest","KNN"),col=c("salmon","green","blue"),lwd=4,cex=0.9,
text.col = c("salmon","green","blue"),bty='n')
plot(knn_model)
knn_model <- train(target ~ D0 + D1 + D3 + D2 + D4 + None , train_data ,
trControl = trainControl,
method = "knn",
tuneLength = 10,
preProcess = c("center","scale"))
knn_cm
#Plot ROC and AUC
roc(test_data$target,logis_pred_roc$blue,plot = T,print.auc=T,col="salmon",main="ROC Curve of 3 models")
roc(test_data$target,randomF_pred_roc$blue,plot = T,print.auc=T,print.auc.y=.4,col="green",add=T)
roc(test_data$target,knn_pred_roc$blue,plot = T,print.auc=T,col="blue",print.auc.y=.3,add=T)
#add legend
legend("bottomright",legend=c("Logistic","Random Forrest","KNN"),col=c("salmon","green","blue"),lwd=4,cex=0.9,
text.col = c("salmon","green","blue"),bty='n')
# Set up the orginal data set
master <- read.csv(choose.files())
# Data Science Lifecycle
#     1. Get The Data -> gather data
#     2. Clean The Data -> handle: missing values, inaccurate data type, wrong observations
#     3. Explore The Data -> check for patterns, trends and transform
#     4. Model the Data -> make predictive models
#     5. Interpret -> explain the results
# set up libarires
packages <- c("dplyr", "plyr","ggformula", "caret", "pROC", "rpart", "rpart.plot", "psych", "car", "stats", "randomForest")
# Install package if there are not on machine
for (p in packages) {
if (require(p)){ install.packages(p) }
}
# Add libraries
lapply(packages, FUN = library, character.only = T)
# Data Science Lifecycle
#     1. Get The Data -> gather data
#     2. Clean The Data -> handle: missing values, inaccurate data type, wrong observations
#     3. Explore The Data -> check for patterns, trends and transform
#     4. Model the Data -> make predictive models
#     5. Interpret -> explain the results
# set up libarires
packages <- c("dplyr", "plyr","ggformula", "caret", "pROC", "rpart", "rpart.plot", "psych", "car", "stats", "randomForest")
# Install package if there are not on machine
for (p in packages) {
if (require(p)){ install.packages(p) }
}
# Add libraries
# lapply(packages, FUN = library, character.only = T)
# Set up the orginal data set
master <- read.csv(choose.files())
# Set up the orginal data set
master <- read.csv("vote_drough.csv",header=T)
#--------- Check for missing values
sum(is.na(master))
#--------- Check for variable data types
str(master)
# Fix data types
master$target_2012 <- as.factor(master$target_2012)
# Set up the orginal data set
master <- read.csv("vote_drough_1.csv",header=T)
#--------- Check for missing values
sum(is.na(master))
#--------- Check for variable data types
str(master)
# Fix data types
master$target_2012 <- as.factor(master$target_2012)
master$target_2016 <- as.factor(master$target_2016)
#--------- Clean Out unwated variables
#  Make Work data set
mentor <- master[,1:8]
mentor$target <- master[,13] # target
# Create Scatter plot matrix
pairs(mentor[,3:9], pch = 21, bg = c("red", "blue")[unclass(mentor[,9])])
# Compute Corrolation matrix
cor(mentor[,3:8]) # **** indepent variables have a high correlation
#----  See variable distributions
# Numerical Variable Frquency Distribution
multi.hist(mentor[,3:8], freq = T, bcol = "red")
library(psych)
#----  See variable distributions
# Numerical Variable Frquency Distribution
multi.hist(mentor[,3:8], freq = T, bcol = "red")
# Create continegncy table
table(mentor[,9], mentor[,2])
str(mentor)
# Make Full model
model.full <- glm(target ~ ., data = mentor[,3:9], family = "binomial")
# compute the vif
vif(model.full)
library(VIF)
str(mentor)
# Make Full model
model.full <- glm(target ~ ., data = mentor[,3:9], family = "binomial")
# compute the vif
vif(model.full)
# Make Full model
model.full <- glm(target ~ ., data = mentor[,3:9], family = "binomial")
# compute the vif
vif(model.full)
model.full
library(car)
# compute the vif
vif(model.full)
# Principal Component Analysis (Normalize variable by setting scale)
pca <- prcomp(mentor[,3:8], scale = T)
# See pca output (enginvector)
pca
# Get pca summary (std, prop variance and cumulative prop)
summary(pca)
# Make line Plot
plot(pca, type = "l", main = "PCA plot") # elbo at 3 select PC1 & PC2 & PC3
# Makee Scree Plot
biplot(pca, scale = 0)
# Makee Scree Plot
biplot(pca, scale = 0)
# Get the PCs
PCs <- round(pca$x[,1:3], 2)
# Get PCs summary
summary(PCs)
# Get PCs summary
summary(PCs)
# SE data distribution
multi.hist(PCs, freq = T, bcol = "red")
# Create New data set with PCs
expert <- cbind(PCs, mentor)
# Set Seed
set.seed(1234)
# Create Training and Test Rows
#rows <- sample(nrow(expert), nrow(expert) * 0.80)
rows <- createDataPartition(expert$target, p = 0.80, list = F)
# Create Test and Train data
x1_train <- expert[rows,]
x1_test <- expert[-rows,]
#--- Check target Frequency
# table(x1_train$target)  # Train
# table(x1_test$target)   # Test
# Data to model
# keep_pc <- c("PC1", "PC2", "PC3","state", "target")
keep <-  c("None", "D0", "D1", "D2", "D3", "D4","state", "target") # Model with None Has better state + D0 + D1
#---- Create Model Binomial
# Make Null model
model.null <- glm(target ~ 1, data = x1_train[keep], family = "binomial")
# Make Full model
model.full <-  glm(target ~ ., data = x1_train[keep], family = "binomial")
# See Data summary
#summary(model.null)
#summary(model.full)
# make Parsimonious Model to get best mod useing forward selection
model.step <- step(model.null, scope = list(lower = model.null, upper = model.full), direction = "forward")
#model.step
# train model
model.glm <- glm(formula = target ~ state + D0 + D1, data = x1_train[keep], family = "binomial")
#model.train_x1
# Set Seed
set.seed(1234)
# Make Prediction
pred.glm <- predict(model.glm, x1_test, type = "response")
# Create confusion marix
pred <- as.factor(ifelse(pred.glm > 0.63, 1, 0))
# Make Confusin matrix
confusionMatrix(data = pred, reference = x1_test$target, positive = "1", mode = "everything")
# Confusion matrix explained
# https://towardsdatascience.com/understanding-confusion-matrix-a9ad42dcfd62
# Kappa explained
# https://thedatascientist.com/performance-measures-cohens-kappa-statistic/
# Set Seed
set.seed(1234)
# Create random forest model
model.forest <- randomForest(target ~ ., data = x1_train[keep], ntree = 500)
library(randomForest)
# Set Seed
set.seed(1234)
# Create random forest model
model.forest <- randomForest(target ~ ., data = x1_train[keep], ntree = 500)
# Make a prediction model
pred.forest <- predict(model.forest, x1_test[keep])
# Get AIC
mean(x1_test$target == pred.forest)
# Confusion matrix
confusionMatrix(data = pred.forest, reference = x1_test$target, positive = "1", mode = "everything")
tunrRF(model.forest)
plot(randomF_model)
print(randomF_model)
plot(model.forest)
res <- tuneRF(x = subset(x1_train[keep], select = -target),
y = x1_train[keep]$target,
ntreeTry = 500)
res <- tuneRF(x = subset(x1_train[keep], select = -target),
y = x1_train[keep]$target,
ntreeTry = 1000)
plot(randomF_model)
print(randomF_model)
plot(randomF_model)
model.forest$mtry
# Create random forest model with default mtry
model.forest <- randomForest(target ~ ., data = x1_train[keep], ntree = 500 , mtry=2)
model.forest
# Create random forest model with default mtry
model.forest <- randomForest(target ~ ., data = x1_train[keep], ntree = 500 , mtry=3)
model.forest
# Create random forest model with default mtry
model.forest <- randomForest(target ~ ., data = x1_train[keep], ntree = 500 , mtry=2)
model.forest
# Data to model
# keep_pc <- c("PC1", "PC2", "PC3","state", "target")
keep <-  c("None", "D0", "D1", "D2", "D3", "D4","state", "target") # Model with None Has better state + D0 + D1
#---- Create Model Binomial
# Make Null model
model.null <- glm(target ~ 1, data = x1_train[keep], family = "binomial")
# Make Full model
model.full <-  glm(target ~ ., data = x1_train[keep], family = "binomial")
# See Data summary
#summary(model.null)
#summary(model.full)
# make Parsimonious Model to get best mod useing forward selection
model.step <- step(model.null, scope = list(lower = model.null, upper = model.full), direction = "forward")
#model.step
# train model
model.glm <- glm(formula = target ~ state + D0 + D1, data = x1_train[keep], family = "binomial")
#model.train_x1
model.step
step(model.null, scope = list(lower = model.null, upper = model.full), direction = "forward")
model.glm <- glm(formula = target ~ state + None + D0 + D1, data = x1_train[keep], family = "binomial")
model.glm
summary(model.glm)
# Set Seed
set.seed(1234)
# Make Prediction
pred.glm <- predict(model.glm, x1_test, type = "response")
# Create confusion marix
pred <- as.factor(ifelse(pred.glm > 0.63, 1, 0))
# Make Confusin matrix
confusionMatrix(data = pred, reference = x1_test$target, positive = "1", mode = "everything")
# Confusion matrix explained
# https://towardsdatascience.com/understanding-confusion-matrix-a9ad42dcfd62
# Kappa explained
# https://thedatascientist.com/performance-measures-cohens-kappa-statistic/
# Data to model
# keep_pc <- c("PC1", "PC2", "PC3", "state", "target")
keep <-  c("None", "D0", "D1", "D2", "D3", "D4", "state", "target") # Model with None Has better state + D0 + D1
# Set seed
set.seed(1234)
# Train tree model
model.tree <- rpart(target~., data = x1_train[keep], method = "class", control = rpart.control(cp = .01, minsplit = 3), model = T)
# Create a Dicision tree
rpart.plot(model.tree, type = 5, fallen.leaves = T)
install.packages("rpart.plot")
library(rpart.plot)
# Create a Dicision tree
rpart.plot(model.tree, type = 5, fallen.leaves = T)
# Make prediction
pred.tree <- predict(model.tree, x1_test, type = "class")
# Make confusion matrix
confusionMatrix(data = pred.tree, reference = x1_test$target, positive = "1", mode = "everything")
# Set Seed
set.seed(1234)
# Create random forest model with default mtry
model.forest <- randomForest(target ~ ., data = x1_train[keep], ntree = 500 , mtry=2)
model.forest
# Make a prediction model
pred.forest <- predict(model.forest, x1_test[keep])
# Get AIC
mean(x1_test$target == pred.forest)
# Confusion matrix
confusionMatrix(data = pred.forest, reference = x1_test$target, positive = "1", mode = "everything")
# Confusion matrix
confusionMatrix(data = pred.forest, reference = x1_test$target, positive = "1", mode = "everything")
# Create random forest model with default mtry
model.forest <- randomForest(target ~ ., data = x1_train[keep], ntree = 500)
model.forest
# Get AIC
mean(x1_test$target == pred.forest)
# Make a prediction model
pred.forest <- predict(model.forest, x1_test[keep])
# Make confusion matrix
confusionMatrix(data = pred.tree, reference = x1_test$target, positive = "1", mode = "everything")
# Create random forest model with default mtry
model.forest <- randomForest(target ~ ., data = x1_train[keep], ntree = 500)
model.forest
# Make a prediction model
pred.forest <- predict(model.forest, x1_test[keep])
# Get AIC
mean(x1_test$target == pred.forest)
# Confusion matrix
confusionMatrix(data = pred.forest, reference = x1_test$target, positive = "1", mode = "everything")
#create trainControl for reuse
trainControl <- trainControl(method = "repeatedcv", number = 5 , repeats = 5)
# Create Model
model.knn <- train(target ~ state + D0 + D1 , x1_train ,
trControl = trainControl,
method = "knn",
tuneLength = 20,
preProcess = c("center","scale"))
#create trainControl for reuse
trainControl <- trainControl(method = "repeatedcv", number = 5 , repeats = 5)
# Create Model
model.knn <- train(target ~ state + D0 + D1 , x1_train ,
trControl = trainControl,
method = "knn",
tuneLength = 10,
preProcess = c("center","scale"))
#test data set
pred.knn <- predict(model.knn, newdata = x1_test, type = "raw")
#Confusion Matrix with test data
confusionMatrix(as.factor(pred.knn), x1_test$target, mode = 'everything')
plot(model.knn)
#Confusion Matrix with test data
confusionMatrix(as.factor(pred.knn), x1_test$target, mode = 'everything')
# Make ROC
roc.glm <- roc(x1_test$target, pred.glm)
roc.tree <- roc(x1_test$target, as.numeric(pred.tree))
roc.forest <- roc(x1_test$target, as.numeric(pred.forest))
roc.knn <- roc(x1_test$target, as.numeric(pred.knn))
# Compute AUC
roc.glm$auc
roc.tree$auc
roc.forest$auc
roc.knn$auc
# PlotROC curve
plot(roc.glm, col = "green", print.auc=T, print.auc.y=.4, main = "Models' ROC Curve")
plot(roc.tree, col = "purple", print.auc=T, print.auc.y=.3, add = T)
plot(roc.forest, col = "brown", print.auc=T, print.auc.y=.2, add = T)
plot(roc.knn, col = "red", print.auc=T, print.auc.y=.1, add = T)
# Set legend
legend("bottomright", legend=c("Logistic", "Tree",
"Forest", "KNN"), col=c("green", "purple", "brown","red"), lwd=4, cex=0.9,
text.col = c("green", "purple", "brown","red"), bty='n')
